{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, trim\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import hour, dayofmonth, weekofyear, month, year, dayofweek\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl_local.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "input_data = config['PATH']['INPUT_DATA']\n",
    "output_data = config['PATH']['OUTPUT_DATA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(df, tab='', msg='', verbose=True):\n",
    "    if verbose:\n",
    "        if msg != '':\n",
    "            print(msg)\n",
    "        df.printSchema()\n",
    "        print(f'count({tab}): {df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    '''Creates a Spark session.\n",
    "\n",
    "    Output:\n",
    "    * spark -- Spark session.\n",
    "    '''\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_song_data(spark, path, limit=10, verbose=False):\n",
    "    '''\n",
    "    Keyword arguments:\n",
    "    * spark         -- Spark session.\n",
    "    * path          -- path to song parquet file\n",
    "    * limit         -- number of songs to be loaded into pandas dataframe\n",
    "    \n",
    "    Output:\n",
    "    * pdf           -- Pandas dataframe\n",
    "    '''\n",
    "    song_path = os.path.join(path, 'songs')\n",
    "    sdf = spark.read.parquet(f\"{song_path}\")\n",
    "    \n",
    "    pdf = sdf.limit(limit)\\\n",
    "             .toPandas()\\\n",
    "             .sort_values(by ='song_id' )\\\n",
    "             .reset_index(drop=True)\n",
    "    \n",
    "    if verbose:\n",
    "        sdf.printSchema()\n",
    "        print(f'song_count: {sdf.count()}')\n",
    "\n",
    "    return pdf\n",
    "\n",
    "def process_song_data(spark, input_data, output_data, verbose=False):    \n",
    "    ''' Compile song json input data from input_data path into:\n",
    "        song_data and artist_data schema and saving these as \n",
    "        parquet files.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    * spark         -- Spark session.\n",
    "    * input_data    -- path to input_data\n",
    "    * output_data   -- path to save parquet output files\n",
    "    * verbose       -- bool to display logs if True\n",
    "    '''\n",
    "    \n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data, 'song_data/*/*/*/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    log(df, 'song_input', verbose=verbose)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    # schema: song_id, title, artist_id, year, duration\n",
    "    song_schema = ['song_id', 'title', 'artist_id', 'year', 'duration']\n",
    "    songs_table = df.filter(df.song_id != '')\\\n",
    "                    .dropDuplicates(['song_id'])\\\n",
    "                    .withColumn('title', trim(df.title)) \\\n",
    "                    .select(song_schema)\\\n",
    "                    .orderBy('song_id')\n",
    "\n",
    "    log(songs_table, 'song_output', verbose=verbose)\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    song_data_output = os.path.join(output_data, 'songs')\n",
    "    songs_table.write.mode('overwrite').partitionBy(\"year\", \"artist_id\").parquet(song_data_output)\n",
    "    \n",
    "    # extract columns to create artists table\n",
    "    # artist_id, name, location, lattitude, longitude\n",
    "    # artist_schema = ['artist_id', 'name', 'location', 'latitude', 'longitude']\n",
    "    artists_table = df.filter(df.artist_id != '')\\\n",
    "                      .dropDuplicates(['artist_id'])\\\n",
    "                      .withColumn('artist_name', trim(df.artist_name))\\\n",
    "                      .select(\n",
    "                        col('artist_id'),\n",
    "                        col('artist_name').alias('name'),\n",
    "                        col('artist_location').alias('location'),\n",
    "                        col('artist_latitude').alias('latitude'),\n",
    "                        col('artist_longitude').alias('longitude')\n",
    "                       )\n",
    "\n",
    "    log(artists_table, 'artists_table', verbose=verbose)\n",
    "        \n",
    "    # write artists table to parquet files\n",
    "    artists_data_output = os.path.join(output_data, 'artists')\n",
    "    artists_table.write.mode('overwrite').parquet(artists_data_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data, verbose=False):\n",
    "    ''' Compile event log json input data from input_data path into:\n",
    "    users, time and songplays table schema and saving these as \n",
    "    parquet files.\n",
    "        \n",
    "    Keyword arguments:\n",
    "    * spark         -- Spark session.\n",
    "    * input_data    -- path to input_data\n",
    "    * output_data   -- path to save parquet output files\n",
    "    * verbose       -- bool to display logs if True\n",
    "    '''\n",
    "\n",
    "    # get filepath to log data file\n",
    "    log_data = os.path.join(input_data, 'log-data/*.json')\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "  \n",
    "    log(df, 'log', verbose=verbose)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "    \n",
    "    # extract columns for users table\n",
    "    # user_columns = ['user_id', 'first_name', 'last_name', 'gender', 'level']\n",
    "    users_table = df.filter(df.userId != '')\\\n",
    "                      .dropDuplicates(['userId'])\\\n",
    "                      .withColumn('firstName', trim(df.firstName))\\\n",
    "                      .withColumn('lastName', trim(df.lastName))\\\n",
    "                      .select(\n",
    "                        col('userId').alias('user_id'),\n",
    "                        col('firstName').alias('first_name'),\n",
    "                        col('lastName').alias('last_name'),\n",
    "                        col('gender'),\n",
    "                        col('level')                    \n",
    "                    )\n",
    "    \n",
    "    log(users_table, 'users', verbose=verbose)\n",
    "    \n",
    "    users_data_output = os.path.join(output_data, 'users')\n",
    "    users_table.write.mode('overwrite').parquet(users_data_output)\n",
    "    \n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000), TimestampType())\n",
    "    df = df.withColumn('time', get_timestamp(df.ts))\n",
    "\n",
    "    df = df.withColumn('start_time', col('ts'))\\\n",
    "           .withColumn('hour', hour(df.time))\\\n",
    "           .withColumn('day', dayofmonth(df.time))\\\n",
    "           .withColumn('week', weekofyear(df.time))\\\n",
    "           .withColumn('month', month(df.time))\\\n",
    "           .withColumn('year', year(df.time))\\\n",
    "           .withColumn('weekday', dayofweek(df.time))\n",
    "    \n",
    "    time_table = df.dropDuplicates(['ts']).orderBy('ts')\n",
    "    \n",
    "    time_table.createOrReplaceTempView('time_table')\n",
    "    time_table = spark.sql('''\n",
    "        SELECT DISTINCT start_time, hour, day, week, month, year, weekday \n",
    "        FROM time_table\n",
    "    ''')\n",
    "    \n",
    "    log(time_table, 'time_table', verbose=verbose)    \n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_data_output = os.path.join(output_data, 'time')\n",
    "    time_table.write.mode('overwrite').partitionBy('year', 'month').parquet(time_data_output)\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_path = os.path.join(output_data, 'songs')\n",
    "    song_df = spark.read.parquet(song_path)\n",
    "    \n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    # songplay_cols = ['songplay_id', 'start_time', 'user_id', 'level', 'song_id', \\\n",
    "    #                  'artist_id', 'session_id', 'location', 'user_agent']\n",
    "\n",
    "    df.withColumn('song', trim(df.song))\n",
    "    df = df.alias('log').join(song_df.alias('song'),col('song.title') == col('log.song'))\n",
    "\n",
    "    songplays_table = df.withColumn('songplay_id', monotonically_increasing_id())\\\n",
    "                        .select(\n",
    "                            col('ts').alias('start_time'),\n",
    "                            col('userId').alias('user_id'),\n",
    "                            'level',\n",
    "                            'song_id',\n",
    "                            'artist_id',\n",
    "                            col('sessionId').alias('session_id'),\n",
    "                            'location',\n",
    "                            col('userAgent').alias('user_agent'),\n",
    "                            'log.year',\n",
    "                            'log.month'\n",
    "                        )\n",
    "\n",
    "    log(songplays_table, 'songplays', verbose=verbose)\n",
    "    \n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_output_path = os.path.join(output_data, 'songplays')\n",
    "    songplays_table.write.mode('overwrite').partitionBy('year', 'month').parquet(songplays_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_song_data(spark, input_data, output_data, False)\n",
    "process_log_data(spark, input_data, output_data, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
